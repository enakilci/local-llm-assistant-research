{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.utils import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for downloading the necessary json files.\n",
    "# download(\"https://huggingface.co/dspy/cache/blob/main/ragqa_arena_tech_corpus.jsonl\")\n",
    "# download(\"https://huggingface.co/dspy/cache/blob/main/ragqa_arena_tech_examples.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to LM from your DSPy code\n",
    "options = {\"num_ctx\": 1024}\n",
    "lm = dspy.LM('ollama_chat/deepseek-r1:7b', api_base='http://localhost:11434', num_ctx= 8192*2)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<think>\\nOkay, so I need to figure out how to respond to the user\\'s request where they asked me to say \"This is a test in Turkish\" three times. Hmm, let\\'s break this down.\\n\\nFirst, I should understand what exactly the user is asking for. They want me to repeat that specific sentence in Turkish three times. So my response needs to be straightforward and repetitive as per their instruction.\\n\\nWait, but why would someone ask that? Maybe they\\'re testing if I can do basic language translation or if I\\'m paying attention. Alternatively, it could be part of a larger task or assessment. Regardless, the immediate request is clear: repeat the sentence three times in Turkish.\\n\\nI should make sure each repetition is correct and uses proper Turkish grammar. Let me think about how \"This is a test in Turkish\" translates into Turkish. The phrase would be \"Bu bir Türkçe test.\" So I need to say that three times accurately.\\n\\nAlso, considering the user might want it in a specific format or with certain punctuations, but since they didn\\'t specify, just repeating it as is should suffice. Maybe adding an emoji could make it friendlier, like a smiley face at the end, which is common in responses here.\\n\\nPutting it all together, I\\'ll structure each sentence correctly and ensure there\\'s no grammatical error. Let me double-check: \"Bu bir Türkçe test.\" Yes, that sounds right. Repeating it three times with proper spacing between them should do the trick.\\n</think>\\n\\nBu bir Türkçe test.  \\nBu bir Türkçe test.  \\nBu bir Türkçe test.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calling LM directly\n",
    "lm(\"Say this is a test!\", temperature=0.7)  # => ['This is a test!']\n",
    "lm(messages=[{\"role\": \"user\", \"content\": \"Say this is a test in Turkish three times.\"}])  # => ['This is a test!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<think>\\nOkay, so I need to say \"This is a test\" in German three times. Hmm, let\\'s think about how to approach this.\\n\\nFirst, I should figure out the correct German phrase for \"This is a test.\" In English, it\\'s pretty straightforward: \"This is a test.\" But in German, it might be slightly different because of their sentence structure and word order.\\n\\nI remember that in German, subjects come before verbs. So maybe instead of saying \"This is a test,\" the German equivalent would start with the subject. Let me think... Oh, right! It\\'s often phrased as \"Das ist ein Test.\" That makes sense because \"das\" means this, \"ist\" is to be, and \"ein Test\" means a test.\\n\\nSo each time I say it, I should repeat \"Das ist ein Test.\" But the user asked for three times. So I just need to write that phrase three separate times in German.\\n\\nWait, but maybe they want me to structure it differently? Like using different tenses or something else? No, the instruction was straightforward: \"Say this is a test in German three times.\"\\n\\nSo yeah, each time, it\\'s simply \"Das ist ein Test.\" Repeating that three times should suffice. I don\\'t think there\\'s any need for variation unless specified.\\n\\nI guess another way to phrase it could be using \"Es ist ein Test,\" but \"das ist ein test\" is more direct and commonly used. So sticking with the first version is better.\\n</think>\\n\\nDas ist ein Test.  \\nDas ist ein Test.  \\nDas ist ein Test.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calling LM directly\n",
    "lm(\"Say this is a test!\", temperature=0.7)  # => ['This is a test!']\n",
    "lm(messages=[{\"role\": \"user\", \"content\": \"Say this is a test in German three times.\"}])  # => ['This is a test!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic DSPy Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning='Each die has six faces (1 to 6). The total number of possible outcomes when tossing both dice is 6 * 6 = 36. Only one outcome (both dice showing 1) results in a sum of two. Therefore, the probability is 1/36 ≈ 0.0278.',\n",
      "    answer=0.0278\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "math = dspy.ChainOfThought(\"question -> answer:float\")\n",
    "output = math(question=\"Two dice are tossed. What is the probability that the sum equals two?Explain to me in detail\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert output to float: The probability that the sum equals two when two dice are tossed is \\boxed{\\dfrac{1}{36}}.\n"
     ]
    }
   ],
   "source": [
    "# Converting the fraction to a float programmatically:\n",
    "import fractions\n",
    "\n",
    "math = dspy.ChainOfThought(\"question -> answer\")\n",
    "output = math(question=\"Two dice are tossed. What is the probability that the sum equals two?\")\n",
    "\n",
    "# Convert fraction to float\n",
    "try:\n",
    "    probability = float(fractions.Fraction(output.answer))\n",
    "    print(probability)  # Expected: 0.027777...\n",
    "except ValueError:\n",
    "    print(\"Could not convert output to float:\", output.answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning=\"To find the probability that the sum of two dice equals two, we first determine the total number of possible outcomes when tossing two dice, which is \\\\(6 \\\\times 6 = 36\\\\). The only favorable outcome for a sum of two is both dice showing 1. Thus, there's 1 favorable outcome out of 36 possible outcomes.\",\n",
      "    answer=0.0278\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "math = dspy.ChainOfThought(\"question -> reasoning:str, answer:float\")\n",
    "\n",
    "output = math(question=\"Two dice are tossed. What is the probability that the sum equals two? \")\n",
    "\n",
    "print(output)  # Should return a structured output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0278"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson\n",
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('ollama_chat/deepseek-r1:7b', api_base='http://localhost:11434', num_ctx= 8192*2)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High memory refers to processes consuming a significant amount of RAM, while low memory indicates underutilization of system resources.\n"
     ]
    }
   ],
   "source": [
    "qa = dspy.Predict('question: str -> response: str')\n",
    "response = qa(question=\"what are high memory and low memory on linux?\")\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot = dspy.ChainOfThought('question -> response')\n",
    "cot(question=\"should curly braces appear on their own line?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
    "    data = [ujson.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'why are my text messages coming up as maybe?', 'response': 'This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \\n\\nHowever, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.', 'gold_doc_ids': [3956, 3957, 8034]}) (input_keys={'question'})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [dspy.Example(**d).with_inputs('question') for d in data]\n",
    "\n",
    "# Let's pick an `example` here from the data.\n",
    "example = data[2]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300, 500)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.Random(0).shuffle(data)\n",
    "trainset, devset, testset = data[:200], data[200:500], data[500:1000]\n",
    "\n",
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \t why are my text messages coming up as maybe?\n",
      "\n",
      "Gold Response: \t This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \n",
      "\n",
      "However, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.\n",
      "\n",
      "Predicted Response: \t Please ensure you're typing correctly and try sending different types of messages to see if the issue persists.\n",
      "\n",
      "Semantic F1 Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "# Instantiate the metric.\n",
    "metric = SemanticF1(decompositional=True)\n",
    "\n",
    "# Produce a prediction from our `cot` module, using the `example` above as input.\n",
    "pred = cot(**example.inputs())\n",
    "\n",
    "# Compute the metric score for the prediction.\n",
    "score = metric(example, pred)\n",
    "\n",
    "print(f\"Question: \\t {example.question}\\n\")\n",
    "print(f\"Gold Response: \\t {example.response}\\n\")\n",
    "print(f\"Predicted Response: \\t {pred.response}\\n\")\n",
    "print(f\"Semantic F1 Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-02-24T14:55:21.433997]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str)\n",
      "2. `ground_truth` (str)\n",
      "3. `system_response` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `ground_truth_key_ideas` (str): enumeration of key ideas in the ground truth\n",
      "3. `system_response_key_ideas` (str): enumeration of key ideas in the system response\n",
      "4. `discussion` (str): discussion of the overlap between ground truth and system response\n",
      "5. `recall` (float): fraction (out of 1.0) of ground truth covered by the system response\n",
      "6. `precision` (float): fraction (out of 1.0) of system response covered by the ground truth\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## ground_truth ## ]]\n",
      "{ground_truth}\n",
      "\n",
      "[[ ## system_response ## ]]\n",
      "{system_response}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## ground_truth_key_ideas ## ]]\n",
      "{ground_truth_key_ideas}\n",
      "\n",
      "[[ ## system_response_key_ideas ## ]]\n",
      "{system_response_key_ideas}\n",
      "\n",
      "[[ ## discussion ## ]]\n",
      "{discussion}\n",
      "\n",
      "[[ ## recall ## ]]\n",
      "{recall}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## precision ## ]]\n",
      "{precision}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Compare a system's response to the ground truth to compute recall and precision of key ideas.\n",
      "        You will first enumerate key ideas in each response, discuss their overlap, and then report recall and precision.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "why are my text messages coming up as maybe?\n",
      "\n",
      "[[ ## ground_truth ## ]]\n",
      "This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \n",
      "\n",
      "However, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.\n",
      "\n",
      "[[ ## system_response ## ]]\n",
      "Please ensure you're typing correctly and try sending different types of messages to see if the issue persists.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## ground_truth_key_ideas ## ]]`, then `[[ ## system_response_key_ideas ## ]]`, then `[[ ## discussion ## ]]`, then `[[ ## recall ## ]]` (must be formatted as a valid Python float), then `[[ ## precision ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m<think>\n",
      "Okay, so I need to figure out how to compute the recall and precision based on the given ground truth and system response. Let me start by understanding what each part means.\n",
      "\n",
      "First, looking at the question: \"why are my text messages coming up as maybe?\" The user is confused because their messages are showing as \"Maybe\" when they're not trying to send something specific.\n",
      "\n",
      "The ground truth explanation says that this feature looks into emails for any mentions of the recipient's number and also checks if contacts were saved from emails. However, there's a bug in iOS 11.2 where \"Maybe\" appears even when \"Find Contacts in Other Apps\" is disabled. So the ground truth key ideas are:\n",
      "1. The feature examines emails for sender info.\n",
      "2. It looks at contacts saved in emails.\n",
      "3. There's a bug causing false positives.\n",
      "\n",
      "The system response provided by the user was: \"Please ensure you're typing correctly and try sending different types of messages to see if the issue persists.\" So the key ideas here are:\n",
      "1. Typing correctly is important.\n",
      "2. Experiment with various message types.\n",
      "\n",
      "Now, I need to find the overlap between these two sets. The ground truth talks about examining emails for contacts, which isn't directly addressed in the system response. Similarly, the bug causing false positives isn't mentioned either. On the other hand, the system response doesn't address the actual issue of \"Maybe\" appearing when it shouldn't.\n",
      "\n",
      "So there's no overlap between the key ideas from ground truth and system response. That means both recall and precision are zero because neither covers any part of each other.\n",
      "</think>\n",
      "\n",
      "The reasoning is that the system response does not address the core issues mentioned in the ground truth, such as examining emails for contacts or the bug causing false positives.\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "The system response does not address the key ideas from the ground truth. The ground truth highlights issues related to email contact checking and a potential bug, while the system response focuses on ensuring correct typing and testing different messages. There is no overlap between these key points.\n",
      "\n",
      "[[ ## ground_truth_key_ideas ## ]]\n",
      "['examine emails for sender info', 'contacts saved in emails', 'bug in iOS 11.2 causing false positives']\n",
      "\n",
      "[[ ## system_response_key_ideas ## ]]\n",
      "['ensure correct typing', 'test various message types']\n",
      "\n",
      "[[ ## discussion ## ]]\n",
      "The system response does not address the issues raised in the ground truth, such as examining emails for contacts or a bug causing false positives. Instead, it focuses on ensuring correct typing and testing different messages.\n",
      "\n",
      "[[ ## recall ## ]]\n",
      "0.0\n",
      "\n",
      "[[ ## precision ## ]]\n",
      "0.0\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an evaluator that we can re-use.\n",
    "evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=24,\n",
    "                            display_progress=True, display_table=2)\n",
    "\n",
    "# Evaluate the Chain-of-Thought program.\n",
    "evaluate(cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install -c rapidsai -c conda-forge -c nvidia libcuvs=24.12 'cuda-version>=12.0,<=12.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson\n",
    "import dspy\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('ollama_chat/deepseek-r1:7b', api_base='http://localhost:11434', num_ctx= 8192*2)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28436 documents. Will encode them below.\n",
      "Training a 32-byte FAISS index with 337 partitions, based on 28436 x 1024-dim embeddings\n"
     ]
    }
   ],
   "source": [
    "max_characters = 6000  # for truncating >99th percentile of documents\n",
    "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "\n",
    "with open(\"ragqa_arena_tech_corpus.jsonl\") as f:\n",
    "    corpus = [ujson.loads(line)['text'][:max_characters] for line in f]\n",
    "    print(f\"Loaded {len(corpus)} documents. Will encode them below.\")\n",
    "\n",
    "\n",
    "# Load an extremely efficient local model for retrieval\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cuda\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = search(question).passages\n",
    "        return self.respond(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning=\"The given context does not provide any information about Emin and Mert's relationship. There is no mention of either individual, nor any indication of a friendship between them.\",\n",
       "    response='Based on the provided context, there is no evidence that Emin is a friend of Mert.'\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag = RAG()\n",
    "rag(question=\"Is Emin a friend of Mert? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-02-24T14:24:22.036385]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `context` (str)\n",
      "2. `question` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `response` (str)\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## context ## ]]\n",
      "{context}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `context`, `question`, produce the fields `response`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «Just stand outside the door at some distance talking on your phone. Dont look at the door, dont look at the person coming to open it, dont look like you want to get in. Dont ask to be let in. Dont engage in conversation. Just let the person open the door and go through. Then in the last second before it closes and lock, you calmly walk through still talking on your phone. Wearing a costume or high-vis will make you... well, highly visible. In some places you might need the costume and the excuse to get in. But in a lot of places, just blending in like an unmemorable nobody is quite enough. Dress like you belong, dont ask, just walk. As a disclaimer I should note that I have no professional experience with this. But I do use it all the time to get into my office when I forget my RFID tag.»\n",
      "[2] «Probably Not Its not an accomplishment but most of all because it puts everything youve ever said under scrutiny. I dont mind telling my peers that Im ignorant about something so I can learn from them but I dont want an employer seeing that. I just cant trust someone in HR to know that someone that can admit they dont know everything is better than someone that thinks they do.»\n",
      "[3] «Dont let them know not to put that in, we need to be able to easily weed out the companies we dont want to work for! To me fast-paced environment = lots of unpaid overtime.»\n",
      "[4] «You need: shopt -s extglob Thanks @Stéphane Chazales for pointing me at extglob.»\n",
      "[5] «Boss-speak is always tough... Zero is a number so you can do things with it. Null is a unicorn. It doesnt exist so you cant do anything at all with it.»\n",
      "[6] «Multi thread, its like having many things to do at a time, and do them little by little. Hey the young curious guy, every day you must eat, play & sleep; right? But why dont you eat once... take a lot until you cant take more, then sleep all the time you need, then just play & play interesting games? Well, you cant do that, because if you dont eat, youll get hungry, no matter how much you got in the previous meal. If you dont sleep, youll be tired and cant play with your friends. Understood? Theres several things that need to be done, but we cant do them at once. We eat breakfast, play game, take lunch, play games again,... and says good night at last. Thats also the way a computer work: they play the music a little, then switch to the picture which you are drawing, then switch again to play music. But the time it took to do all of those things a thousand time is just a wink of eye, so you cant see it.»\n",
      "[7] «Im not sure what my real name is anymore. Seriously: there are people I know mainly in real life, who I rarely talk to online, who still know me as TRiG. (Theyd know my real name too, usually.) And there are people I know mainly or exclusively online whod still know my real name. The two identities blur together, especially since the nickname TRiG is in fact derived from my initials. You could find my online postings across a lot of forums very easily if you found a case-sensitive search engine.»\n",
      "[8] «curl prints its status to stderr rather than stdout. To capture stderr in the same file, you need to redirect stderr to stdout by adding 2>&1 AFTER your stdout redirection: curl -T /home/pi/fb/$DATE.jpg ftp://myftpserver --user myuser:mypass >> /home/pi/fb/log.txt 2>&1 For a thought-provoking question that will help you understand stdout and stderr better... why must the 2>&1 be after the initial >> instead of before?»\n",
      "[9] «http://en.wikipedia.org/wiki/Design_pattern_%28computer_science%29 - is a good enough list. I wouldnt say thats a lot. Picking what design pattern you are gonna remember is like picking whether you will carry a hammer or a screw driver. They dont replace each other, they complement each other.»\n",
      "[10] «Try a Terminal Emulator app: $ date Fri Mar 27 16:00:15 IDT 2015»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "Is Emin a friend of Mert?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m<think>\n",
      "Okay, so I need to figure out if Emin is a friend of Mert based on the given context. Let me start by looking through each piece of information provided.\n",
      "\n",
      "First, in point [2], it says \"Probably Not Its not an accomplishment but most of all because it puts everything you've ever said under scrutiny.\" Hmm, that doesn't seem directly related to Emin or Mert.\n",
      "\n",
      "Point [3] mentions \"Dont let them know not to put that in, we need to be able to easily weed out the companies we dont want to work for! To me fast-paced environment = lots of unpaid overtime.\" Again, no mention of Emin or Mert here.\n",
      "\n",
      "Looking at point [4], it's about a tool called shopt with some URL and a thank you note. Not relevant either.\n",
      "\n",
      "Point [5] talks about \"Boss-speak is always tough...\" which seems to be discussing communication styles rather than people.\n",
      "\n",
      "In point [6], there's a long explanation about multitasking, but no names are mentioned here either.\n",
      "\n",
      "Point [7] discusses the user's confusion with their online identities and mentions TRiG. Wait, TRiG could be a username or handle name. Maybe Emin is TRiG? But I don't see any direct connection to Mert there.\n",
      "\n",
      "Point [8] is about curl commands in programming, so definitely not relevant here.\n",
      "\n",
      "Point [9] talks about design patterns in computer science and how they complement each other. Again, no names involved.\n",
      "\n",
      "Finally, point [10] mentions a terminal emulator app with some date info. No connection to Emin or Mert either.\n",
      "\n",
      "So after going through all the points, I don't see any information that links Emin or Mert together as friends. All the context provided doesn't mention both individuals in relation to each other.\n",
      "</think>\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "The given context does not provide any information about Emin and Mert's relationship. There is no mention of either individual, nor any indication of a friendship between them.\n",
      "\n",
      "[[ ## response ## ]]\n",
      "Based on the provided context, there is no evidence that Emin is a friend of Mert.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning=\"High Memory (also known as Application Space) is reserved for user applications to prevent interference with the kernel. Low Memory contains the kernel's essential data, which cannot be accessed or swapped out, making it a critical resource for system stability and performance.\",\n",
       "    response=\"High memory refers to physical RAM allocated exclusively for user applications, ensuring they don't interfere with the kernel. Low memory is where the kernel resides, containing essential data that can't be swapped out or used by applications. Applications should aim to use high memory as much as possible to avoid relying on low memory, which can cause performance issues.\"\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag = RAG()\n",
    "rag(question=\"what are high memory and low memory on linux? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-02-24T14:27:13.713408]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `context` (str)\n",
      "2. `question` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `response` (str)\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## context ## ]]\n",
      "{context}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `context`, `question`, produce the fields `response`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «Reading system memory usage in Activity Monitor from support.apple.com gives a detailed explanation about the different types of RAM. Free memory: This is RAM thats not being used. Wired memory: Information in this memory cant be moved to the hard disk, so it must stay in RAM. The amount of Wired memory depends on the applications you are using. Active memory: This information is currently in memory, and has been recently used. Inactive memory: This information in memory is not actively being used, but was recently used. Used: This is the total amount of memory used.»\n",
      "[2] «You ideally want both banks the same, this will allow interleaving which almost doubles the memory access speed. In computing, interleaved memory is a design made to compensate for the relatively slow speed of dynamic random-access memory (DRAM) or core memory, by spreading memory addresses evenly across memory banks. That way, contiguous memory reads and writes are using each memory bank in turn, resulting in higher memory throughputs due to reduced waiting for memory banks to become ready for desired operations. Source : http://en.wikipedia.org/wiki/Interleaved_memory»\n",
      "[3] «Real mem relates to physical memory (actual RAM modules in your computer). Virtual Mem is how much fake memory is allocated to the process, meaning memory that is allocated on the permanent storage medium (hard drive, solid state drive, etc) for that process. Shared memory is physical (Real) memory that can be shared with other processes. Private memory is real memory that can only be used by the process it is allocated to. These explanations may help as well... directly from activity monitor --> help --> viewing system memory usage: Here is an explanation of some of the information displayed at the bottom of the memory pane: Wired: Wired memory contains information that must always stay in RAM Active: Active memory that contains information that is actively being used. Inactive: Inactive memory contains information that is not actively being used. Leaving this information in RAM is to your advantage if you (or a client of your computer) come back to it later. Used: Used memory is being used by a process or by the system. Used memory is the sum of wired, active, and inactive memory. If the system requires memory it takes free memory before used memory. Free: Free memory is not being used and is immediately available. VM size: Virtual memory, or VM, is hard disk space that can be used as memory. VM size is the amount of disk space being used as memory. Mac OS X can use more memory than the amount of physical RAM you have. A hard disk is much slower than RAM, so the virtual memory system automatically distributes information between disk space and RAM for efficient performance. Page ins/outs: The number of gigabytes of information Mac OS X has moved between RAM and disk space»\n",
      "[4] «As far as I remember, High Memory is used for application space and Low Memory for the kernel. Advantage is that (user-space) applications cant access kernel-space memory.»\n",
      "[5] «Theres a key difference between managing memory as a resource (when you dont care whats inside and shouldnt even look), and using memory to do something else (when the contents are the whole point). ... memory administration is often better accomplished ... The quote is talking about managing memory addressing and maps, where youre treating memory as some opaque resource to be managed. The code you posted isnt managing memory as a resource, its using some memory to do non-memory-management-related stuff (specifically DMA to an audio device).»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "what are high memory and low memory on linux?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m<think>\n",
      "Okay, so I need to figure out what High Memory and Low Memory mean on Linux. Let me start by recalling some basic knowledge about memory management in operating systems.\n",
      "\n",
      "I remember that in many systems, including Linux, the memory is divided into different regions based on their usage and purpose. High Memory usually refers to a portion of physical RAM that's allocated specifically for user-space applications. This means that these applications can't access or interfere with this part of the memory because it's protected from the kernel (the core of the operating system). So, when an application needs more memory than is available in high memory, it has to use low memory.\n",
      "\n",
      "On the other hand, Low Memory seems to be a region where the kernel resides. Since the kernel manages the system, it can't be swapped out or used by applications. Therefore, if an application tries to run and doesn't have enough high memory, it might need to access low memory, which is not ideal because it's meant for the kernel.\n",
      "\n",
      "I also recall that managing these regions properly is important for performance and stability. If there's insufficient high memory, applications might start using low memory, leading to potential issues like increased latency or even crashes if too many processes try to use low memory simultaneously.\n",
      "\n",
      "So putting this together, High Memory is dedicated for user apps, while Low Memory holds the kernel. Applications should aim to use as much high memory as possible to avoid relying on low memory, which can cause problems.\n",
      "</think>\n",
      "\n",
      "High Memory refers to a portion of physical RAM allocated exclusively for user-space applications, ensuring they don't interfere with the kernel. Low Memory is where the kernel resides and cannot be swapped out or used by applications.\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "High Memory (also known as Application Space) is reserved for user applications to prevent interference with the kernel. Low Memory contains the kernel's essential data, which cannot be accessed or swapped out, making it a critical resource for system stability and performance.\n",
      "\n",
      "[[ ## response ## ]]\n",
      "High memory refers to physical RAM allocated exclusively for user applications, ensuring they don't interfere with the kernel. Low memory is where the kernel resides, containing essential data that can't be swapped out or used by applications. Applications should aim to use high memory as much as possible to avoid relying on low memory, which can cause performance issues.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='MVC stands for Model-View-Controller. It is a design pattern that separates an application into three main components: the model, which handles data; the view, which presents data to the user; and the controller, which manages user interactions. This separation of concerns allows for better maintainability and modularity.',\n",
       "    response='MVC stands for Model-View-Controller. It is a design pattern that separates an application into three main components: the model, which handles data; the view, which presents data to the user; and the controller, which manages user interactions. This separation of concerns allows for better maintainability and modularity.'\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag = RAG()\n",
    "rag(question=\"What is MVC? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-02-24T14:22:14.628784]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `context` (str)\n",
      "2. `question` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `response` (str)\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## context ## ]]\n",
      "{context}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `context`, `question`, produce the fields `response`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «Dont let them know not to put that in, we need to be able to easily weed out the companies we dont want to work for! To me fast-paced environment = lots of unpaid overtime.»\n",
      "[2] «Boss-speak is always tough... Zero is a number so you can do things with it. Null is a unicorn. It doesnt exist so you cant do anything at all with it.»\n",
      "[3] «http://en.wikipedia.org/wiki/Design_pattern_%28computer_science%29 - is a good enough list. I wouldnt say thats a lot. Picking what design pattern you are gonna remember is like picking whether you will carry a hammer or a screw driver. They dont replace each other, they complement each other.»\n",
      "[4] «Try a Terminal Emulator app: $ date Fri Mar 27 16:00:15 IDT 2015»\n",
      "[5] «Oren Eini aka Ayende Rahien, author of Rhino Mocks and other great open source tools. His is some of the best and most elegant code around.»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "Is Emin a friend of Mert?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m<think>\n",
      "Okay, so I'm trying to figure out if Emin is a friend of Mert based on the given context. Let me start by looking through each piece of information provided.\n",
      "\n",
      "First, in point [1], it mentions that someone doesn't want to work with certain companies because they have fast-paced environments leading to unpaid overtime. That seems unrelated to the question about Emin and Mert.\n",
      "\n",
      "Point [2] talks about boss-speak and design patterns like null and zero. Again, not directly related to the names Emin and Mert.\n",
      "\n",
      "Point [3] is a link to an article about design patterns in computer science. I'm not sure how that connects either; maybe it's just providing context but doesn't answer the question.\n",
      "\n",
      "Point [4] mentions a Terminal Emulator app with some date information, which also seems irrelevant.\n",
      "\n",
      "Point [5] is about Oren Eini, known for creating tools like Rhino Mocks. Still, this doesn't help in determining if Emin and Mert are friends.\n",
      "\n",
      "Wait, maybe I missed something. The names mentioned here are \"Emin,\" \"Mert,\" \"Oren Eini,\" \"Rhino Mocks,\" etc. Oh, wait! Oren Eini is the author of Rhino Mocks. So perhaps Emin refers to Oren Eini? Or is Emin a different person?\n",
      "\n",
      "If Emin is Oren Eini, then Mert might be another name or perhaps someone else entirely. But without more context about who Emin and Mert are, it's hard to say if they're friends.\n",
      "\n",
      "Alternatively, maybe \"Emin\" is a username or part of a handle for someone else. Without additional information linking Emin and Mert together, I can't confirm their relationship based solely on the given data.\n",
      "\n",
      "So, considering all this, there's no direct evidence provided that connects Emin and Mert as friends.\n",
      "</think>\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "The context provided does not contain any specific information about individuals named Emin or Mert. The only mention of \"Emin\" is in relation to Oren Eini, who is known for creating tools like Rhino Mocks. There is no indication that Emin refers to Oren Eini or anyone else, and thus there is no evidence linking Emin and Mert as friends.\n",
      "\n",
      "[[ ## response ## ]]\n",
      "Based on the provided context, there is insufficient information to determine if Emin is a friend of Mert.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
    "    data = [ujson.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'why are my text messages coming up as maybe?', 'response': 'This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \\n\\nHowever, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.', 'gold_doc_ids': [3956, 3957, 8034]}) (input_keys={'question'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [dspy.Example(**d).with_inputs('question') for d in data]\n",
    "\n",
    "# Let's pick an `example` here from the data.\n",
    "example = data[2]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300, 500)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.Random(0).shuffle(data)\n",
    "trainset, devset, testset = data[:200], data[200:500], data[500:1000]\n",
    "\n",
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "# Instantiate the metric.\n",
    "metric = SemanticF1(decompositional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an evaluator that we can re-use.\n",
    "evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=24,\n",
    "                         display_progress=True, display_table=2)\n",
    "\n",
    "# Evaluate the Chain-of-Thought program.\n",
    "evaluate(RAG())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with ColBERTv2 ( Wikipedia Searcher Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_wikipedia(query: str) -> list[str]:\n",
    "    results = dspy.ColBERTv2(url='https://huggingface.co/dspy/cache/blob/main/wiki.abstracts.2017.tar.gz')(query, k=3)\n",
    "    return [x['text'] for x in results]\n",
    "\n",
    "rag = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "question = \"What's the name of the castle that David Gregory inherited?\"\n",
    "rag(context=search_wikipedia(question), question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        user_agent=\"LLM-Assistant/1.0 (ecagatay93@email.com)\"\n",
    "    )\n",
    "    page = wiki_wiki.page(query)\n",
    "\n",
    "    if not page.exists():\n",
    "        return \"No relevant Wikipedia page found.\"\n",
    "\n",
    "    return page.text[:10000]  # Limit content for processing\n",
    "\n",
    "# Example usage\n",
    "# question = \"For which artist did Kendrick Lamar write the diss track 'Not Like Us'?\"\n",
    "context = search_wikipedia(\"2024 United States presidential election\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning=\"The context mentions that Donald Trump ran for re-election in 2024 and was nominated along with his running mate, JD Vance. The article does not mention any other candidates as Trump's running mate.\",\n",
       "    response='JD Vance'\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "question = \"What is the name of running mate of Donald Trump in 2024?\"\n",
    "rag(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='The context provided mentions that Donald Trump falsely claimed there was voter fraud in the 2020 presidential election, which he denied after losing. This includes his baseless predictions about voter fraud in 2024 as well as his claims of election interference.',\n",
       "    response='Yes, Trump claimed that the 2020 election was rigged and made false claims about voter fraud to deny the results.'\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "question = \"Did trump claim that the 2020 elections were rigged?\"\n",
    "rag(context=context, question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a DSPy Optimizer to improve your RAG prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('ollama_chat/deepseek-r1:7b', api_base='http://localhost:11434', num_ctx= 8192*2)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = dspy.MIPROv2(metric=metric, auto=\"medium\", num_threads=24)  # use fewer threads if your rate limit is small\n",
    "\n",
    "optimized_rag = tp.compile(RAG(), trainset=trainset,\n",
    "                            max_bootstrapped_demos=2, max_labeled_demos=2,\n",
    "                            requires_permission_to_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = rag(question=\"cmd+tab does not work on hidden or minimized windows\")\n",
    "print(baseline.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = optimized_rag(question=\"cmd+tab does not work on hidden or minimized windows\")\n",
    "print(pred.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(optimized_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = sum([x['cost'] for x in lm.history if x['cost'] is not None])  # in USD, as calculated by LiteLLM for certain providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_rag.save(\"optimized_rag.json\")\n",
    "\n",
    "loaded_rag = RAG()\n",
    "loaded_rag.load(\"optimized_rag.json\")\n",
    "\n",
    "loaded_rag(question=\"cmd+tab does not work on hidden or minimized windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    sentiment='positive',\n",
       "    confidence=0.8\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class Classify(dspy.Signature):\n",
    "    \"\"\"Classify sentiment of a given sentence.\"\"\"\n",
    "\n",
    "    sentence: str = dspy.InputField()\n",
    "    sentiment: Literal['positive', 'negative', 'neutral'] = dspy.OutputField()\n",
    "    confidence: float = dspy.OutputField()\n",
    "\n",
    "classify = dspy.Predict(Classify)\n",
    "classify(sentence=\"This book was super fun to read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractInfo(dspy.Signature):\n",
    "    \"\"\"Extract structured information from text.\"\"\"\n",
    "\n",
    "    text: str = dspy.InputField()\n",
    "    title: str = dspy.OutputField()\n",
    "    headings: str = dspy.OutputField(desc=\"a JSON string representing a list of headings\")\n",
    "    entities: str = dspy.OutputField(desc=\"a JSON string representing a list of entities and their metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "module = dspy.Predict(ExtractInfo)\n",
    "text = \"Apple Inc. announced its latest iPhone 14 today.\" \\\n",
    "    \"The CEO, Tim Cook, highlighted its new features in a press release.\"\n",
    "response = module(text=text)\n",
    "\n",
    "print(\"Raw response:\", repr(response))\n",
    "print(response.title)\n",
    "print(response.headings)\n",
    "print(response.entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractInfo(dspy.Signature):\n",
    "    \"\"\"Extract structured information from text.\"\"\"\n",
    "\n",
    "    text: str = dspy.InputField()\n",
    "    title: str = dspy.OutputField()\n",
    "    headings: list[str] = dspy.OutputField()\n",
    "    entities: list[dict[str, str]] = dspy.OutputField(desc=\"a list of entities and their metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: Prediction(\n",
      "    title='Submerged Vessel Incident Near Sag Harbor Entrance',\n",
      "    headings=['Vessel Identification', 'Position', 'Mariner Caution'],\n",
      "    entities=[{'name': 'Vessel Tag', 'value': 'tag zero one decimal eight two north zero seven two'}, {'name': 'Position', 'value': 'four one tag zero one decimal eight two north zero seven two west'}, {'name': 'Direction', 'value': 'zero zero west'}, {'name': 'Warning', 'value': 'break the coast guard recieved a report of a vessel that is submerged in the vicinity of Sag Harbor entrance'}]\n",
      ")\n",
      "Submerged Vessel Incident Near Sag Harbor Entrance\n",
      "['Vessel Identification', 'Position', 'Mariner Caution']\n",
      "[{'name': 'Vessel Tag', 'value': 'tag zero one decimal eight two north zero seven two'}, {'name': 'Position', 'value': 'four one tag zero one decimal eight two north zero seven two west'}, {'name': 'Direction', 'value': 'zero zero west'}, {'name': 'Warning', 'value': 'break the coast guard recieved a report of a vessel that is submerged in the vicinity of Sag Harbor entrance'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "module = dspy.Predict(ExtractInfo)\n",
    "text = \"securite securite all stations this is United States Coast Guard Sector Long Island Sound break the coast guard recieved a report of a vessel that is submerged in the vicinity of Sag Harbor entrance in broad position four one tag zero one decimal eight two north zero seven two tag one eight decimal zero zero west all mariners are requested caution when transiting the area break this is United States Coast Guard Sector Long Island Sound out\"\n",
    "response = module(text=text)\n",
    "\n",
    "print(\"Raw response:\", repr(response))\n",
    "print(response.title)\n",
    "print(response.headings)\n",
    "print(response.entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractInfo(dspy.Signature):\n",
    "    \"\"\"Extract structured information from text.\"\"\"\n",
    "\n",
    "    text: str = dspy.InputField()\n",
    "    title: str = dspy.OutputField()\n",
    "    headings: str = dspy.OutputField(desc=\"a JSON string representing a list of headings\")\n",
    "    entities: str = dspy.OutputField(desc=\"a JSON string representing a list of entities and their metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: Prediction(\n",
      "    title='Kotka VTS',\n",
      "    headings='[\\n  \"Kotka VTS\",\\n  \"Vessel Strandvej\"\\n]',\n",
      "    entities='[\\n  {\\n    \"name\": \"Kotka VTS\",\\n    \"type\": \"ship_name\",\\n    \"description\": \"A vessel named Kotka VTS.\"\\n  },\\n  {\\n    \"name\": \"Onki\",\\n    \"type\": \"person_name\",\\n    \"description\": \"A person\\'s name, Onki.\"\\n  },\\n  {\\n    \"name\": \"Tavasland\",\\n    \"type\": \"location_or_area\",\\n    \"description\": \"A location or area named Tavasland.\"\\n  }\\n]'\n",
      ")\n",
      "Kotka VTS\n",
      "[\n",
      "  \"Kotka VTS\",\n",
      "  \"Vessel Strandvej\"\n",
      "]\n",
      "[\n",
      "  {\n",
      "    \"name\": \"Kotka VTS\",\n",
      "    \"type\": \"ship_name\",\n",
      "    \"description\": \"A vessel named Kotka VTS.\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Onki\",\n",
      "    \"type\": \"person_name\",\n",
      "    \"description\": \"A person's name, Onki.\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Tavasland\",\n",
      "    \"type\": \"location_or_area\",\n",
      "    \"description\": \"A location or area named Tavasland.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "module = dspy.Predict(ExtractInfo)\n",
    "text = \"Kotka VTS, Onki, hyvää iltapäivää. Vessel Strandvej, this is Tavasland.\"\n",
    "response = module(text=text)\n",
    "\n",
    "print(\"Raw response:\", repr(response))\n",
    "print(response.title)\n",
    "print(response.headings)\n",
    "print(response.entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_math(expression: str):\n",
    "    return dspy.PythonInterpreter({}).execute(expression)\n",
    "\n",
    "def search_wikipedia(query: str):\n",
    "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n",
    "    return [x['text'] for x in results]\n",
    "\n",
    "react = dspy.ReAct(\"question -> answer: float\", tools=[evaluate_math, search_wikipedia])\n",
    "\n",
    "pred = react(question=\"What is 9362158 divided by the year of birth of David Gregory of Kinnairdy castle?\")\n",
    "print(pred.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Stage Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.1 ('llm-assistant')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f5d55a1ce58292716dc692e51d545a364fdcbf96d2173792e35ede8640fb9dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
